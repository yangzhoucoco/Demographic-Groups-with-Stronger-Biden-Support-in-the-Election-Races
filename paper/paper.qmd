---
title: "My title"
subtitle: "My subtitle if needed"
author: 
  - First author
  - Another author
thanks: "Code and data are available at: <https://github.com/yangzhoucoco/Political-support-in-the-United-States.git>."
date: today
date-format: long
abstract: 
There is no other nation in the world that conducts elections the way the United States does. Local government representatives oversee elections in around 8,000 distinct jurisdictions. They are then under the general supervision of authorities, who are frequently selected through election procedures or partisan appointees. Typically, the voter liaison at the polling place is a temporary worker who works unpaid for a single day and only gets a few hours of training. These distinctive characteristics of our electoral system, along with the fact that Americans vote more frequently than citizens in other countries on a wider range of issues and positions, create special difficulties for the efficient administration of elections that voters across demand and deserve.
None of the other nations' electoral systems display all or any of these traits, but none have the same mix of US administrative traits. The quality of administration differs from jurisdiction to jurisdiction and even from polling place to polling place because of decentralization and dependence on volunteers. Because partisan officials will be involved, accusations of partisanship regarding the rules or their interpretation may arise based on the outcome of the judgments made by the officials and who stands to gain from it. The quantity and variety of democratic options in protracted elections have outpaced voters' attention spans and cognitive capacities. These distinguishing characteristics of our voting system contribute to the issues seen in the most recent elections. This paper is mainly based on the 2022 US election.
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(here)
```


# Introduction
The goal of the Cooperative Election Study (CES) is to investigate how Americans vote and perceive their electoral experiences, how they hold their representatives accountable in elections, and how their behavior and experiences vary depending on the political climate and social milieu. The study used an extremely large sample that allowed it to account for variations across different legislative districts. Actually, the sample size at the state level is sufficient to measure the voter preference distribution within the majority of states with a respectable degree of accuracy.
60 teams participated in 2022CES, which resulted in a 60,000-case generic content sample. 
Fall of 2022 saw the recruitment of study participants. Every research team invested in a 1,000-person nationwide sample survey from Redwood City, California-based YouGov. Two waves of interviewing were used for the 2022 survey. Pre-election surveys were filled out on the spot between September 29 and November 8. The post-election campaigning frenzy took place between November 10 and December 15. For every 1,000 respondents, half of the questions had common material while the other half were fully created and managed by each study team. Questions that are common to all team modules and have a sample size equal to the total sample size of all team modules make up common content. Every one of the sixty teams bought a 1,000-person survey. Every case was chosen online, and YouGov created a corresponding random sample for this investigation.
The first data release happened on March 20, 2023. The Harvard University database contains archived and accessible data from this investigation. Vote verification is included in data release 2 for all respondents.
The study on CES 2022 is still under progress. Using a large-scale national survey, the Collaborative Congressional Election project was founded in 2006 to investigate congressional elections and representation. The project built upon the analysis conducted by the Public Opinion Research and Training Lab at the Massachusetts Institute of Technology in 2005.
# Data {#sec-data}
For the actual data we can use the 2022 Cooperative Election Study (CES) (@main_data)

## Source and Methodology

## Sampling and Sampling Matching
The CES 2022 dataset is a sample of adult U.S. citizens, taken from a considerably larger population, rather than an exhaustive list of all possible cases in the target population. The sample was deliberately created to be a true reflection of the population of the United States and to capture the variations among the different legislative districts for in-depth examination in the majority of states.
Through the Internet, YouGov conducted the CES 2022 survey. 60,000 adults were interviewed between September and November 2022 (pre-election data) and November and December 2022 (post-election data) for the Common Opinion. The YouGov matching approach for random sampling was used. YouGov employed the matched random sampling method at CES among other sampling techniques. The first step in this process is to count the target population, which in the case of general population research consists of all adults. This is typically done by using an extensive survey such as the American Community Survey. The target sample is then selected at random by CES from this target population. 
However, because it is frequently impractical to make direct contact with these individuals, a matching sample is chosen from among selectable respondents who share the same characteristics as the target sample. This selection process is carried out based on a multitude of attributes available in the voter and consumer databases.
## Weightening
A weighting process is used to account for any residual imbalances between the matched samples, therefore rigorously verifying the representativeness of CES samples. The weighting of the sample was adjusted to align with the framework's distribution along several demographic and political aspects. Entropy balancing and iterative proportional fitting (sequencing) of multiple important variables and their interactions comprise the first stage of the weighting process. The sample is also thought to be representative of every state for the survey's common content and takes into account changes for statewide political races.
There are two steps involved in selecting samples when utilizing the matching approach. First, select a sample at random from the intended audience. This sample is known as the target sample. The process of choosing a "representative" sample from a non-random sample of responders is known as sample matching. Although it works well for Web access panels, it may also be applied to other survey kinds, such phone surveys. The target population is enumerated before sample matching begins. All adults are the target group for general population studies, and they can be counted using either a top-notch survey like the American Community Survey or the decennial Census. This is referred to as a sampling frame in other contexts, although unlike traditional sampling, the sample is not taken out of the frame. 
Using traditional sampling, participants in the study are chosen at random from a framework for sampling. Because not every person in the framework has access to contact information, particularly email addresses, and because declining to participate would raise the expense of sampling in this manner, it could not be practical or cost-effective.
##Vote Vaildation

## Variables
```{r}
#| label: fig-variables
#| fig-cap: Brief summary of variables types
#| echo: false
#| warning: false
#| message: false

ces2022 <-  read_csv(file = here::here("data/raw_datas/raw_datas.csv"))
    col_types =
      cols(
        "votereg" = col_integer(),
        "presvote20post" = col_integer(),
        "gender" = col_integer(),
        "race" = col_integer()
      )
ces2022
```
  After downloading @dataverse, We using @get_dataframe_by_name to access the CES. We select and save the data that we are most care about, which are "votereg", "presvote20post", "race", and "gender4". "votereg" represent the voter registration status. To be more specific, whether they are registered to vote or not. "presvote20post" records the president that voter select in the election of 2020. “race" describe the voter's raical or ethnic group, and "gender4" refer to the gender of voters.
  However, when we access the raw data, there are some variables that we are not interested in. Therefore, we use the codebook to delve into the details. We only care about the voter who are registered to vote, and we are mainly focus on investigating the voter who select Biden or Trump in 2020. We found out that when the "presvote20post" is 1, then this indicates the register vote for Biden, and when it is 2 represents that the voter stand for Trump. We use @tidyverse to filter to the voters that we care about and label them with meaningful title. In addition，CES also provide the information for the gender of the voter. However, we noticed that there are four types of gender, which are man, woman, non-binary, other, and none and we are more interested in the group of man and woman. Therefore, we filter the gender by using @tidyverse. When the variable "gender" is 1, this indicates "man", but we rename all "man" to "male". When the variable "gender" is 2, it refers to "woman". We also rename all "woman" to "female". Finally, the codebook also define different "race" from 1 to 8, which are White, Black, Hispanic, Asian, Native American, Middle Eastern, Two or more races, and other. We keep all those races because we are not only interested in the major racial group, but also curious about the respond from minority group.
  To better understand the raw data, a summary table had been drawn in order to provide more details for each variables, explaining the variables we select.
  
## Measurements
```{r}
#| label: fig-vote
#| fig-cap: The distribution of presidential preferences, by gender, and race
#| echo: false
#| warning: false
#| message: false

ces2022 <- read_csv(here("data/analysis_data/analysis_data.csv"))

ces2022 |>
  ggplot(aes(x = race, fill = voted_for)) +
  stat_count(position = "dodge") +
  geom_text(stat='count', aes(label=..count..), position=position_dodge(width=0.9), vjust=-0.25, color="black") +
  facet_wrap(facets = vars(gender4)) +
  theme_minimal() +
  labs(
    x = "Race",
    y = "Number of respondents",
    fill = "Voted for"
  ) +
  coord_flip() +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "bottom")
```



Talk more about it.

And also planes (@fig-vote). (You can change the height and width, but don't worry about doing that until you have finished every other aspect of the paper - Quarto will try to make it look nice and the defaults usually work well once you have enough text.)


Talk way more about it. 



# Model

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ is the political preference of the respondent and equal to 1 if Biden and 0 if Trump. Then $\mbox{gender}_i$ is the gender of the respondent and $\mbox{race}_i$ is the race of the respondent.  
We could estimate the parameters using `stan_glm()`. Note that the model is a generally accepted short-hand.  In practice rstanarm converts categorical variables into a series of indicator variables and there are multiple coefficients estimated. In the interest of run-time we will randomly sample 500 observations and fit the model on that, rather than the full dataset.



\begin{align} 
y_i|\pi_i &\sim \mbox{Bern}(\pi_i) \\
\mbox{logit}(\pi_i) & = \beta_0+\beta_1 \times\mbox{gender}_i +\beta_2 \times \mbox{education}_i\\
\beta_0 &\sim \mbox{Normal}(0, 2.5)\\
\beta_1 &\sim \mbox{Normal}(0, 2.5)\\
\beta_2 &\sim \mbox{Normal}(0. 2.5)
\end{align}



We run the model in R [@citeR] using the `rstanarm` package of @rstanarm, `here` package of @here and `model_summary` package of @model_summary, we use the default priors from `rstanarm`


### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.


# Results

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

us_political_preferences <-
  readRDS(file = here::here("model/first_model.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

modelsummary(
  list(
    "Support Biden" = us_political_preferences
  ),
  statistic = "mad"
)

```




# Discussion

## First discussion point {#sec-first-point}
Logistic regression is also known as generalized linear regression model, and its form is basically the same as that of linear regression model. The biggest difference lies in their different dependent variables. If it is continuous, it is multiple linear regression. If it is a binomial distribution, it is Logistic regression.
Although Logistic regression has the name "regression", it is actually a classification method, mainly used for binary classification problems (that is, there are only two outputs, each representing two categories), but also can handle multiple classification problems.
Linear regression is used to predict continuous variables, whose value range is (- ", + "), while logistic regression model is used to predict categories. For example, using logistic regression model to predict whether an item belongs to class A or Class B essentially predicts the probability that the item belongs to class A or class B, and the value range of probability is 0~1. Therefore, it is not possible to predict the probability directly with the linear regression equation, which involves the Sigmoid function, which converts the values of the range (-∞, +∞) to the range (0,1).

## Second discussion point
In summary, the essence of logistic regression model is to transform the linear regression model Q through a nonlinear Sigmoid function to obtain a probability value between 0 and 1. For binary classification problem (classification 0 and 1), the probability of predicting the classification as 1(or the classification with a larger value in the binary classification) can be calculated using the formula shown below. df has a total of about 7000 groups of historical data, of which about 2000 groups are lost customers, about 5000 groups are not lost customers will "whether to lose" as the target variable, other fields as characteristic variables, through some basic information and transaction records of a customer to predict whether he will lose, or judge the probability of loss.

## Third discussion point
The training set is used to train the data and build the model, and the test set is used to check the effect of the model built after training. The purpose of dividing the training set and the test set is to evaluate the model, and to optimize the model through the test set. Dividing the training set and the test set is also, in part, to check for overfitting of the model.

## Weaknesses and next steps



\newpage


# References


